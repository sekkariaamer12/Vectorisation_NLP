{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd4ca573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, collections\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd497b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Topic    Stance  \\\n",
      "0           avion     other   \n",
      "1           avion     other   \n",
      "2           avion     other   \n",
      "3    d√©croissance  positive   \n",
      "4       nucl√©aire     other   \n",
      "..            ...       ...   \n",
      "495     nucl√©aire  positive   \n",
      "496         other        -1   \n",
      "497        viande  negative   \n",
      "498        viande  negative   \n",
      "499         other        -1   \n",
      "\n",
      "                                                  Text           Author  \n",
      "0    üá∏üá¨ Un ancien Airbus A380 de Singapore Airlines...      airplusnews  \n",
      "1    Deux Dassault Falcon 900LX pour la Royal Air¬†F...       AeroBuzzfr  \n",
      "2    üî¥ Isra√´l vient d'annoncer l'interdiction des v...      airplusnews  \n",
      "3    Mais ce volet l√† est particuli√®rement importan...      salomesaque  \n",
      "4    Ce n‚Äôest pas le cas de #Malte, qui a peu de pl...  voixdunucleaire  \n",
      "..                                                 ...              ...  \n",
      "495  La baisse des √©missions carbone, c'est l'√©lect...      emma_ducros  \n",
      "496  ... J'y vois une 1√®re impasse.\\nAvec le plan a...       GeWoessner  \n",
      "497  Dans une nouvelle enqu√™te #L214 montre des cas...             L214  \n",
      "498  Entre les transactivistes myopes qui s'abonnen...     HypathieBlog  \n",
      "499  ‚û°Ô∏è Le gaz contribue √† la #CriseClimatique\\n‚û°Ô∏è ...     greenpeacefr  \n",
      "\n",
      "[500 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Read the JSON file\n",
    "with open('results.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create an empty list to store the rows\n",
    "rows = []\n",
    "\n",
    "# Iterate over each key-value pair in the JSON data\n",
    "for key, value in data.items():\n",
    "    # Extract the required fields from the value\n",
    "    topic = value.get('topic')\n",
    "    stance = value.get('stance')\n",
    "    text = value.get('text')\n",
    "    author = value.get('author')\n",
    "\n",
    "    # Create a row as a list with the extracted fields\n",
    "    row = [topic, stance, text, author]\n",
    "\n",
    "    # Add the row to the list of rows\n",
    "    rows.append(row)\n",
    "\n",
    "# Create the DataFrame using the list of rows\n",
    "df = pd.DataFrame(rows, columns=['Topic', 'Stance', 'Text', 'Author'])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286bbb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Text    Stance\n",
      "0    üá∏üá¨ Un ancien Airbus A380 de Singapore Airlines...     other\n",
      "1    Deux Dassault Falcon 900LX pour la Royal Air¬†F...     other\n",
      "2    üî¥ Isra√´l vient d'annoncer l'interdiction des v...     other\n",
      "3    Mais ce volet l√† est particuli√®rement importan...  positive\n",
      "4    Ce n‚Äôest pas le cas de #Malte, qui a peu de pl...     other\n",
      "..                                                 ...       ...\n",
      "495  La baisse des √©missions carbone, c'est l'√©lect...  positive\n",
      "496  ... J'y vois une 1√®re impasse.\\nAvec le plan a...        -1\n",
      "497  Dans une nouvelle enqu√™te #L214 montre des cas...  negative\n",
      "498  Entre les transactivistes myopes qui s'abonnen...  negative\n",
      "499  ‚û°Ô∏è Le gaz contribue √† la #CriseClimatique\\n‚û°Ô∏è ...        -1\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove \"topic\" and \"author\" columns\n",
    "df = df.drop(['Topic', 'Author'], axis=1)\n",
    "\n",
    "# Rearrange the columns\n",
    "df = df[['Text', 'Stance']]\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "532e0bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Stance'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d9d6182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üá∏üá¨ Un ancien Airbus A380 de Singapore Airlines...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deux Dassault Falcon 900LX pour la Royal Air¬†F...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üî¥ Isra√´l vient d'annoncer l'interdiction des v...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mais ce volet l√† est particuli√®rement importan...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ce n‚Äôest pas le cas de #Malte, qui a peu de pl...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text    Stance\n",
       "0  üá∏üá¨ Un ancien Airbus A380 de Singapore Airlines...     other\n",
       "1  Deux Dassault Falcon 900LX pour la Royal Air¬†F...     other\n",
       "2  üî¥ Isra√´l vient d'annoncer l'interdiction des v...     other\n",
       "3  Mais ce volet l√† est particuli√®rement importan...  positive\n",
       "4  Ce n‚Äôest pas le cas de #Malte, qui a peu de pl...     other"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9b7d4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "836b7b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texte</th>\n",
       "      <th>Avis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üá∏üá¨ Un ancien Airbus A380 de Singapore Airlines...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deux Dassault Falcon 900LX pour la Royal Air¬†F...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üî¥ Isra√´l vient d'annoncer l'interdiction des v...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mais ce volet l√† est particuli√®rement importan...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ce n‚Äôest pas le cas de #Malte, qui a peu de pl...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>On a √©t√© dans un des seuls resto un peu gastro...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>@BShirizadeh @rogerseban @BridonneauV @Reclaim...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>La baisse des √©missions carbone, c'est l'√©lect...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Dans une nouvelle enqu√™te #L214 montre des cas...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Entre les transactivistes myopes qui s'abonnen...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Texte      Avis\n",
       "0    üá∏üá¨ Un ancien Airbus A380 de Singapore Airlines...     other\n",
       "1    Deux Dassault Falcon 900LX pour la Royal Air¬†F...     other\n",
       "2    üî¥ Isra√´l vient d'annoncer l'interdiction des v...     other\n",
       "3    Mais ce volet l√† est particuli√®rement importan...  positive\n",
       "4    Ce n‚Äôest pas le cas de #Malte, qui a peu de pl...     other\n",
       "..                                                 ...       ...\n",
       "492  On a √©t√© dans un des seuls resto un peu gastro...  negative\n",
       "494  @BShirizadeh @rogerseban @BridonneauV @Reclaim...     other\n",
       "495  La baisse des √©missions carbone, c'est l'√©lect...  positive\n",
       "497  Dans une nouvelle enqu√™te #L214 montre des cas...  negative\n",
       "498  Entre les transactivistes myopes qui s'abonnen...  negative\n",
       "\n",
       "[194 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers=df.rename(columns = {'Text': 'Texte', 'Stance' : 'Avis'}) \n",
    "df_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b92fdd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Texte', 'Avis'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66f8732b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texte</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avis</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Texte\n",
       "Avis           \n",
       "negative     65\n",
       "other        45\n",
       "positive     84"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers.groupby(['Avis']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f31140c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(df_papers[[\"Texte\"]], df_papers['Avis'], train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29c84d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56b74137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "297d7e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\amese\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "082baf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "# Une seule fois :\n",
    "if False : # Si d√©j√† fit\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('words')\n",
    "    nltk.download('punkt')    \n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('brown')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('omw-1.4')   \n",
    "    \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "import string\n",
    "\n",
    "# Initialisation du \"Wordnet Lemmatizer\"\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#Pour tester :  print(lemmatizer.lemmatize(\"bats\"))\n",
    "\n",
    "\n",
    "from nltk.corpus import brown  # Il y a davantage de mots ici\n",
    "words = set(brown.words())\n",
    "\n",
    "#words = set(nltk.corpus.words.words()) : pas beaucoup de mots !\n",
    "stop_words=set(stopwords.words('french')); # \";\" pour ne pas avoir les r√©sultats !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61610e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(texte) :\n",
    "    #renvoie lemmatizer.lemmatize(texte)\n",
    "    return [lemmatizer.lemmatize(t) for t in word_tokenize(texte) if \\\n",
    "            t.lower() in words and \\\n",
    "            t.lower() not in stop_words \\\n",
    "            # cas des strs sp√©cifiques non filtr√©s\n",
    "            and t not in [\"''\", '--', '1.2', '1/2', '18th', '2-3', '20th', '4.00', '4.2', '``', 'le', 'u']\\\n",
    "            #and t.lower() not in word_tokenize(stop_words).encode() \\ # g√©n√®re un pb de 'byte' ?!\n",
    "            and t.lower() not in string.punctuation and not t.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "328d0f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amese\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a  262  termes dans le vocabulaire\n",
      "\n",
      "Les 50 premiers termes:\n",
      "['1/3' '2/4' 'abandon' 'action' 'ad' 'aggressor' 'ah' 'aide' 'air'\n",
      " 'airline' 'alliance' 'alors' 'amp' 'animal' 'anticipation' 'article'\n",
      " 'assurance' \"aujourd'hui\" 'avant' 'aviation' 'baron' 'base' 'bien'\n",
      " 'breaking' 'candidate' 'capital' 'car' 'cargo' 'cause' 'certain'\n",
      " 'certification' 'chance' 'charge' 'chart' 'chef' 'chose' 'chute'\n",
      " 'citation' 'cite' 'civil' 'clean' 'code' 'coin' 'collection' 'combat'\n",
      " 'combustible' 'come' 'comment' 'commercial' 'commission']\n",
      "\n",
      "Tous les 50 termes:\n",
      "['1/3' 'compare' 'genre' 'million' 'question' 'tropical']\n",
      "\n",
      "Le nbr d'occurrence des 50 premiers termes :\n",
      "[('airline', 9), ('hunt', 110), ('deux', 61), ('falcon', 83), ('royal', 214), ('air', 8), ('force', 93), ('provenance', 196), ('pay', 175), ('important', 118), ('car', 26), ('propose', 194), ('solution', 226), ('tout', 243), ('suite', 235), ('transport', 247), ('place', 180), ('jour', 130), ('plus', 183), ('aide', 7), ('production', 191), ('comment', 47), ('impact', 115), ('cause', 28), ('plan', 181), ('grandeur', 102), ('grand', 101), ('service', 221), ('avant', 18), ('bien', 22), ('point', 184), ('anticipation', 14), ('suspend', 239), ('mobile', 153), ('candidate', 24), ('capital', 25), ('difficile', 62), ('2/4', 1), ('manifestation', 142), ('normal', 160), ('cite', 38), ('ready', 204), ('route', 213), ('premier', 189), ('segment', 219), ('nutrition', 162), ('dire', 63), ('imbecile', 114), ('partisan', 171), ('explosion', 79)]\n",
      "\n",
      "Le nbr d'occurrence des 50 premiers termes :\n",
      "[('spring', 229), ('alliance', 10), ('loin', 139), ('titre', 241), ('baron', 20), ('repose', 210), ('eating', 69), ('impatient', 116), ('article', 15), ('rare', 203), ('maximum', 144), ('parole', 170), ('humaine', 109), ('hors', 108), ('stable', 230), ('correspondent', 57), ('flexible', 88), ('energy', 72), ('chart', 33), ('pro', 190), ('punitive', 198), ('formation', 94), ('patron', 174), ('conversation', 55), ('niger', 158), ('vrai', 259), ('improbable', 120), ('profit', 192), ('expert', 77), ('jeunes', 129), ('russe', 216), ('positive', 186), ('hurler', 111), ('chance', 31), ('video', 256), ('clean', 40), ('sky', 224), ('initial', 122), ('emission', 71), ('collection', 43), ('fake', 82), ('eh', 70), ('compatible', 51), ('coin', 42), ('ad', 4), ('pure', 199), ('usage', 253), ('hypocrite', 112), ('troll', 249), ('plaque', 182)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amese\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vec_lemmatise = CountVectorizer(tokenizer=lemma, stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=0, max_features=None)\n",
    "\n",
    "# Transformer les donn√©es en  bag of words\n",
    "count_train = count_vec_lemmatise.fit(df_papers[\"Texte\"])\n",
    "bag_of_words_of_corpus = count_vec_lemmatise.transform(df_papers[\"Texte\"])\n",
    "\n",
    "# On enl√®ve qq termes inutiles qui nous ont √©chapp√©s (qui ont √©t√© cr√©√©s par lemmatize)\n",
    "# Il s'agit d'un Dict de Python.\n",
    "for terme_a_jeter in ['n', 'u', 'ft'] :\n",
    "    count_train.vocabulary_.pop(terme_a_jeter, terme_a_jeter+\" n'y est ps !\")\n",
    "\n",
    "\n",
    "# Quelques prints \n",
    "print(\"Il y a \", len(count_train.vocabulary_), \" termes dans le vocabulaire\\n\")\n",
    "\n",
    "# Print Les 50 premiers termes\n",
    "print(\"Les 50 premiers termes:\\n{}\".format(np.array(count_vec_lemmatise.get_feature_names_out()[:50])))\n",
    "print(\"\\nTous les 50 termes:\\n{}\".format(np.array(count_vec_lemmatise.get_feature_names_out()[::50]))) # Tous les 50 termes\n",
    "#print(type(count_train.vocabulary_))\n",
    "#print(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_))\n",
    "\n",
    "#les 50 premiers mots et leur nbr d'occurrence\n",
    "print(\"\\nLe nbr d'occurrence des 50 premiers termes :\")\n",
    "print([(k,v)  for k,v in count_train.vocabulary_.items()][:50])\n",
    "\n",
    "print(\"\\nLe nbr d'occurrence des 50 premiers termes :\")\n",
    "print([(k,v)  for k,v in count_train.vocabulary_.items()][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee851a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 50 premiers termes:\n",
      "['abandon' 'abattoir' 'abord' 'abyssal' 'ad' 'aggressor' 'ah' 'aide' 'air'\n",
      " 'alimentation' 'allemand' 'aller' 'alliance' 'amour' 'animal'\n",
      " 'anticipation' 'apache' 'assurer' 'augmenter' 'austral' 'aviation' 'ba'\n",
      " 'base' 'belle' 'bien' 'bon' 'bord' 'bouche' 'bravo' 'break' 'bure' 'ca'\n",
      " 'canada' 'candidate' 'capital' 'car' 'cargo' 'cause' 'centrale' 'certain'\n",
      " 'certification' 'chance' 'changement' 'changer' 'charbon' 'charge'\n",
      " 'chasse' 'chauffer' 'chef' 'choose']\n",
      "\n",
      "Tous les 50 termes:\n",
      "['abandon' 'chute' 'exploitation' 'jean' 'opulence' 'repose' 'utile']\n",
      "\n",
      "Le nbr d'occurrence des 50 premiers termes :\n",
      "[('hunt', 134), ('falcon', 105), ('royal', 255), ('air', 8), ('force', 115), ('provenance', 230), ('volet', 307), ('important', 140), ('car', 35), ('propose', 228), ('tout', 289), ('suite', 279), ('place', 212), ('si', 264), ('plu', 215), ('aide', 7), ('production', 225), ('comment', 62), ('cause', 37), ('soja', 269), ('retour', 252), ('assurer', 17), ('semble', 261), ('doit', 86), ('continuer', 71), ('centrale', 38), ('plan', 213), ('charbon', 44), ('explication', 99), ('droit', 88), ('grandeur', 127), ('solution', 271), ('grand', 126), ('bravo', 28), ('service', 263), ('bien', 24), ('point', 216), ('anticipation', 15), ('suspend', 283), ('mobile', 182), ('chasse', 46), ('candidate', 33), ('san', 258), ('capital', 34), ('difficile', 81), ('dont', 87), ('dit', 84), ('manifestation', 165), ('normal', 190), ('cite', 52)]\n"
     ]
    }
   ],
   "source": [
    "# On a d√©j√† fait le n√©cessaire plus hatut. \n",
    "# On met un \"if False\" pour ne pas ex√©cuter sauf ...\n",
    "# Eventuellement NE pas utiliser : plus lourd √† installer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "if True : \n",
    "    # Il y a des probl√®mes avec la ligne suivante sur certaine machines\n",
    "    from pattern.en import lemma,lexeme\n",
    "    import nltk\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    class LemmaTokenizer(object):\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "        def __call__(self, articles):\n",
    "            #return [lemma(t) for t in word_tokenize(articles) if t.lower() in words]\n",
    "\n",
    "            return [lemma(t) for t in word_tokenize(articles) if t.lower() in words and \\\n",
    "                    t.lower() not in stop_words \\\n",
    "                    #and t.lower() not in word_tokenize(stop_words).encode() \\ # g√©n√®re un pb de 'byte' ?!\n",
    "                    # cas des strs sp√©cifiques non filtr√©s\n",
    "                    and t not in [\"''\", '--', '1.2', '1/2', '18th', '2-3', '20th', '4.00', '4.2', '``']\\\n",
    "                    and t.lower() not in string.punctuation and not t.isdigit()]\n",
    "\n",
    "        \n",
    "        \n",
    "    words = set(nltk.corpus.words.words())\n",
    "\n",
    "    count_vec_lemmatise = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=\"english\", analyzer='word', \n",
    "                                ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n",
    "\n",
    "    # Transforms the data into a bag of words\n",
    "    count_train = count_vec_lemmatise.fit(df_papers[\"Texte\"])\n",
    "    bag_of_words = count_vec_lemmatise.transform(df_papers[\"Texte\"])\n",
    "\n",
    "    # Print the first 10 features of the count_vec\n",
    "    print(\"Les 50 premiers termes:\\n{}\".format(count_vec_lemmatise.get_feature_names_out()[:50]))\n",
    "    print(\"\\nTous les 50 termes:\\n{}\".format(count_vec_lemmatise.get_feature_names_out()[::50])) # Tous les 50 termes\n",
    "    #print(type(count_train.vocabulary_))\n",
    "    #print(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_))\n",
    "\n",
    "    #les 50 premiers mots et leur nbr d'occurrence\n",
    "    print(\"\\nLe nbr d'occurrence des 50 premiers termes :\")\n",
    "    print([(k,v)  for k,v in count_train.vocabulary_.items()][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd4f5da2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Les 10 derni√®res lignes de la  matrice TfIdf (en ligne : les indices, en colonne les termes/mots) :\n",
      "     abandon  abattoir  abord  abyssal   ad  aggressor   ah  aide       air  \\\n",
      "145      0.0       0.0    0.0      0.0  0.0   0.000000  0.0   0.0  0.000000   \n",
      "146      0.0       0.0    0.0      0.0  0.0   0.000000  0.0   0.0  0.000000   \n",
      "147      0.0       0.0    0.0      0.0  0.0   0.000000  0.0   0.0  0.000000   \n",
      "148      0.0       0.0    0.0      0.0  0.0   0.000000  0.0   0.0  0.000000   \n",
      "149      0.0       0.0    0.0      0.0  0.0   0.000000  0.0   0.0  0.000000   \n",
      "150      0.0       0.0    0.0      0.0  0.0   0.000000  0.0   0.0  0.000000   \n",
      "151      0.0       0.0    0.0      0.0  0.0   0.000000  0.0   0.0  0.000000   \n",
      "152      0.0       0.0    0.0      0.0  0.0   0.000000  0.0   0.0  0.000000   \n",
      "153      0.0       0.0    0.0      0.0  0.0   0.000000  0.0   0.0  0.000000   \n",
      "154      0.0       0.0    0.0      0.0  0.0   6.267858  0.0   0.0  4.321948   \n",
      "\n",
      "     alimentation  ...  vent  vert  via  video  virgule  vise  vol  volet  \\\n",
      "145           0.0  ...   0.0   0.0  0.0    0.0      0.0   0.0  0.0    0.0   \n",
      "146           0.0  ...   0.0   0.0  0.0    0.0      0.0   0.0  0.0    0.0   \n",
      "147           0.0  ...   0.0   0.0  0.0    0.0      0.0   0.0  0.0    0.0   \n",
      "148           0.0  ...   0.0   0.0  0.0    0.0      0.0   0.0  0.0    0.0   \n",
      "149           0.0  ...   0.0   0.0  0.0    0.0      0.0   0.0  0.0    0.0   \n",
      "150           0.0  ...   0.0   0.0  0.0    0.0      0.0   0.0  0.0    0.0   \n",
      "151           0.0  ...   0.0   0.0  0.0    0.0      0.0   0.0  0.0    0.0   \n",
      "152           0.0  ...   0.0   0.0  0.0    0.0      0.0   0.0  0.0    0.0   \n",
      "153           0.0  ...   0.0   0.0  0.0    0.0      0.0   0.0  0.0    0.0   \n",
      "154           0.0  ...   0.0   0.0  0.0    0.0      0.0   0.0  0.0    0.0   \n",
      "\n",
      "     world  zone  \n",
      "145    0.0   0.0  \n",
      "146    0.0   0.0  \n",
      "147    0.0   0.0  \n",
      "148    0.0   0.0  \n",
      "149    0.0   0.0  \n",
      "150    0.0   0.0  \n",
      "151    0.0   0.0  \n",
      "152    0.0   0.0  \n",
      "153    0.0   0.0  \n",
      "154    0.0   0.0  \n",
      "\n",
      "[10 rows x 316 columns]\n",
      "--------------------------------------------------\n",
      "2) Quelques indice Idf pour certains mots :\n",
      "          idf_weights\n",
      "abandon      6.267858\n",
      "abattoir     5.574711\n",
      "abord        6.267858\n",
      "abyssal      6.267858\n",
      "ad           6.267858\n",
      "...               ...\n",
      "vise         5.169246\n",
      "vol          4.658420\n",
      "volet        6.267858\n",
      "world        6.267858\n",
      "zone         6.267858\n",
      "\n",
      "[316 rows x 1 columns]\n",
      "--------------------------------------------------\n",
      "2bis) Le 100 premiers termes : colonnes de la matrice TfIdf\n",
      "['abandon' 'abattoir' 'abord' 'abyssal' 'ad' 'aggressor' 'ah' 'aide' 'air'\n",
      " 'alimentation' 'allemand' 'aller' 'alliance' 'also' 'amour' 'animal'\n",
      " 'anticipation' 'apache' 'assurer' 'augmenter' 'austral' 'aviation' 'ba'\n",
      " 'base' 'belle' 'bien' 'bon' 'bord' 'bouche' 'bravo' 'break' 'bure' 'ca'\n",
      " 'canada' 'candidate' 'capital' 'car' 'cargo' 'cause' 'centrale' 'certain'\n",
      " 'certification' 'chance' 'changement' 'changer' 'charbon' 'charge'\n",
      " 'chasse' 'chauffer' 'chef' 'choose' 'chute' 'citation' 'cite' 'citer'\n",
      " 'civil' 'clean' 'code' 'coin' 'collection' 'combat' 'combustible' 'come'\n",
      " 'comment' 'commercial' 'commission' 'compare' 'compatible' 'compter'\n",
      " 'concept' 'condition' 'continue' 'continuer' 'conviction' 'correspondent'\n",
      " 'corsair' 'corse' 'coup' 'covid' 'cyclone' 'danger' 'dernier' 'difficile'\n",
      " 'dire' 'disposition' 'dit' 'documentary' 'doit' 'dont' 'droit' 'durable'\n",
      " 'eat' 'eh' 'emission' 'encore' 'energy' 'engagement' 'ensemble'\n",
      " 'eternity' 'exhibition']\n",
      "--------------------------------------------------\n",
      "3) Le 100 derniers termes de la matrice = colonnes de la matrice\n",
      "['zone' 'world' 'volet' 'vol' 'vise' 'virgule' 'video' 'via' 'vert' 'vent'\n",
      " 'utile' 'urgence' 'type' 'tweet' 'tropical' 'trompe' 'tribunal' 'travail'\n",
      " 'transport' 'transition' 'train' 'tout' 'tour' 'top' 'toit' 'titre'\n",
      " 'terrine' 'tarn' 'suspend' 'surtout' 'surface' 'super' 'suite' 'sub'\n",
      " 'steak' 'star' 'stable' 'spring' 'source' 'sortie' 'solution' 'sol'\n",
      " 'soja' 'social' 'sky' 'site' 'simple' 'si' 'service' 'sensible' 'semble'\n",
      " 'segment' 'sang' 'san' 'saint' 'rupture' 'royal' 'route' 'rich' 'retour'\n",
      " 'retard' 'repose' 'reportage' 'rend' 'remarque' 'religion' 'relation'\n",
      " 'regarder' 'refuse' 'recension' 'ready' 'rare' 'rapport' 'rappel'\n",
      " 'question' 'pure' 'pur' 'punitive' 'puissance' 'public' 'pu' 'provenance'\n",
      " 'protection' 'propose' 'propagation' 'profit' 'production' 'pro'\n",
      " 'prendre' 'premier' 'pouce' 'pot' 'possible' 'positive' 'porter' 'point'\n",
      " 'plu' 'plaque' 'plan' 'place']\n",
      "--------------------------------------------------\n",
      "Il y a 2400 documents, 3204 termes \n",
      "4) La matrice sur X_train : \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "--------------------------------------------------\n",
      "5) Les 50 premiers termes du vocabulaire  et leur indices :\n",
      "[('hunt', 136), ('falcon', 106), ('royal', 259), ('air', 8), ('force', 116), ('provenance', 234), ('volet', 313), ('important', 143), ('car', 36), ('propose', 232), ('tout', 294), ('suite', 283), ('place', 216), ('si', 268), ('plu', 219), ('aide', 7), ('production', 229), ('comment', 63), ('cause', 38), ('soja', 273), ('retour', 256), ('via', 308), ('assurer', 18), ('semble', 265), ('doit', 87), ('continuer', 72), ('centrale', 39), ('plan', 217), ('charbon', 45), ('explication', 100), ('droit', 89), ('grandeur', 129), ('solution', 275), ('grand', 128), ('bravo', 29), ('service', 267), ('bien', 25), ('point', 220), ('anticipation', 16), ('suspend', 287), ('mobile', 185), ('chasse', 47), ('candidate', 34), ('san', 262), ('capital', 35), ('difficile', 82), ('dont', 88), ('dit', 85), ('manifestation', 168), ('normal', 193)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amese\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['might', 'must', 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stop_words = stopwords.words('english') + stopwords.words('french')\n",
    "TfIdf_lemmatise = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words=stop_words,\\\n",
    "                                  smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\n",
    "# max_df = min_df : par default=1.0\n",
    "\n",
    "corpus_fitted = TfIdf_lemmatise.fit(df_papers[\"Texte\"])\n",
    "train_lemmatise_transformed = corpus_fitted.transform(X_train[\"Texte\"])\n",
    "test_lemmatise_transformed = corpus_fitted.transform(X_test[\"Texte\"])\n",
    "\n",
    "# Une partie de la matrice TDIDF\n",
    "temp_df=pd.DataFrame(train_lemmatise_transformed.toarray(), columns=TfIdf_lemmatise.get_feature_names_out())\n",
    "\n",
    "#print(temp_df.columns.values)\n",
    "print(\"1) Les 10 derni√®res lignes de la  matrice TfIdf (en ligne : les indices, en colonne les termes/mots) :\")\n",
    "print(temp_df.tail(10))\n",
    "print( \"-\"*50)\n",
    "\n",
    "df_idf = pd.DataFrame(TfIdf_lemmatise.idf_, index=TfIdf_lemmatise.get_feature_names_out(),columns=[\"idf_weights\"])\n",
    "print(\"2) Quelques indice Idf pour certains mots :\")\n",
    "print(df_idf)\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"2bis) Le 100 premiers termes : colonnes de la matrice TfIdf\")\n",
    "liste_termes=TfIdf_lemmatise.get_feature_names_out()\n",
    "print(liste_termes[:100])\n",
    "print(\"-\"*50)\n",
    "\n",
    "\n",
    "print(\"3) Le 100 derniers termes de la matrice = colonnes de la matrice\")\n",
    "liste_termes = TfIdf_lemmatise.get_feature_names_out()\n",
    "reversed_liste_termes = liste_termes[::-1]\n",
    "print(reversed_liste_termes[:100])\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"Il y a 2400 documents, 3204 termes \")\n",
    "print (\"4) La matrice sur X_train : \\n\", train_lemmatise_transformed.toarray())\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"5) Les 50 premiers termes du vocabulaire  et leur indices :\")\n",
    "print([(k,v)  for k,v in TfIdf_lemmatise.vocabulary_.items()][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daba6d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "if True :\n",
    "    # On a fait le travaille plus haut et d√©j√† cr√©er fit_train et fit_test\n",
    "    pipe = make_pipeline(CountVectorizer(), TfidfTransformer()) # Construire un Construct a Pipeline from the given estimators.\n",
    "    pipe.fit(X_train['Texte'])\n",
    "\n",
    "    fit_train = pipe.transform(X_train['Texte'])\n",
    "    fit_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28262f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 1839)\n"
     ]
    }
   ],
   "source": [
    "fit_test = pipe.transform(X_test['Texte'])\n",
    "print(fit_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670d5ff",
   "metadata": {},
   "source": [
    "# Algrithmes de classification des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73ab3eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48717948717948717"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(fit_train, y_train)\n",
    "dt.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a42d9cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=50)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "clf.fit(fit_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7414a36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5128205128205128"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4baa2e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 1839)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5da8bca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46153846153846156"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(fit_train, y_train)\n",
    "lr.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5788a5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46153846153846156"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(fit_train, y_train)\n",
    "mnb.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0c87722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48717948717948717"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf20 = RandomForestClassifier(n_estimators=120, max_depth=20)\n",
    "clf20.fit(fit_train, y_train)\n",
    "clf20.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1e1f03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf1,10  : score =  0.358974358974359\n",
      "clf1,30  : score =  0.46153846153846156\n",
      "clf1,50  : score =  0.48717948717948717\n",
      "clf1,70  : score =  0.4358974358974359\n",
      "clf1,90  : score =  0.3076923076923077\n",
      "clf51,10  : score =  0.41025641025641024\n",
      "clf51,30  : score =  0.38461538461538464\n",
      "clf51,50  : score =  0.41025641025641024\n",
      "clf51,70  : score =  0.48717948717948717\n",
      "clf51,90  : score =  0.5128205128205128\n",
      "clf101,10  : score =  0.38461538461538464\n",
      "clf101,30  : score =  0.4358974358974359\n",
      "clf101,50  : score =  0.41025641025641024\n",
      "clf101,70  : score =  0.5128205128205128\n",
      "clf101,90  : score =  0.48717948717948717\n",
      "clf151,10  : score =  0.48717948717948717\n",
      "clf151,30  : score =  0.48717948717948717\n",
      "clf151,50  : score =  0.46153846153846156\n",
      "clf151,70  : score =  0.5384615384615384\n",
      "clf151,90  : score =  0.46153846153846156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clfs=[(\"clf\"+str(N)+','+str(depth), RandomForestClassifier(n_estimators=N, max_depth=depth))  for N in range(1,200,50) for depth in range(10,100,20)]\n",
    "for nom, _clf_ in clfs :\n",
    "    _clf_.fit(fit_train, y_train)\n",
    "    print(nom, ' : score = ', _clf_.score(fit_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2c08022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.358974358974359"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc40 = GradientBoostingClassifier(n_estimators=200, max_depth=40)\n",
    "gbc40.fit(fit_train, y_train)\n",
    "gbc40.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6d9be84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5641025641025641"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "modele_one_vs_linear_SVC= OneVsRestClassifier(LinearSVC())\n",
    "modele_one_vs_linear_SVC.fit(fit_train, y_train)\n",
    "modele_one_vs_linear_SVC.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9438e8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46153846153846156"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "modele_one_vs_SVC = OneVsRestClassifier(SVC())\n",
    "modele_one_vs_SVC.fit(fit_train, y_train)\n",
    "modele_one_vs_SVC.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "df5c14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s'il faut t√©l√©charger des donn√©es\n",
    "# \n",
    "nltk_fait = True # A mettre d√®s le premier download\n",
    "if not nltk_fait :\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk_fait == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bed83f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, None, 'RT'),\n",
       " (None, 'RT', '@'),\n",
       " ('RT', '@', 'fmomboisse'),\n",
       " ('@', 'fmomboisse', ':'),\n",
       " ('fmomboisse', ':', 'Pour'),\n",
       " (':', 'Pour', 'pallier'),\n",
       " ('Pour', 'pallier', 'l')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "generated_ngrams = ngrams(word_tokenize(X_train.iloc[0,0]), 3, pad_left=True, pad_right=True)\n",
    "list(generated_ngrams)[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "807a9040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155, 5633)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "pipe2 = make_pipeline(CountVectorizer(ngram_range=(1, 2)), TfidfTransformer())\n",
    "pipe2.fit(X_train['Texte'])\n",
    "fit_train2 = pipe2.transform(X_train['Texte'])\n",
    "fit_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c613a538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['6mw61er4hp', '6qk1s52nyj', '71', '71 il', '750',\n",
       "       '750 scientifiques', '777', '777 300er', '7r4lhah7n7', '7x'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl = pipe2.steps[0]\n",
    "cl[1].get_feature_names_out()[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6e8c64ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 5633)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_test2 = pipe2.transform(X_test['Texte'])\n",
    "fit_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "645fdf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155, 576)\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english') + stopwords.words('french')\n",
    "count_vec_lemmatise = CountVectorizer(tokenizer=lemma, stop_words=stop_words, analyzer='word', \n",
    "                                ngram_range=(1, 2), max_df=1.0, min_df=1, max_features=None)\n",
    "pipe2_bis = make_pipeline(CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=\"english\", analyzer='word', \n",
    "                                ngram_range=(1, 2), max_df=1.0, min_df=1, max_features=None), TfidfTransformer())\n",
    "pipe2_bis.fit(X_train['Texte'])\n",
    "fit_train2_bis = pipe2_bis.transform(X_train['Texte'])\n",
    "print(fit_train2_bis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "39c8082e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 576)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_test2_bis = pipe2_bis.transform(X_test['Texte'])\n",
    "fit_test2_bis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c2ae3c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf2 = LogisticRegression(solver='lbfgs')\n",
    "clf2.fit(fit_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6d400de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6410256410256411"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.score(fit_test2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "09158e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(cv=5)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf2_ = LogisticRegressionCV(cv=5)\n",
    "clf2_.fit(fit_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f519c9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5641025641025641"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2_.score(fit_test2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5d1af541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6153846153846154"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(fit_train2, y_train)\n",
    "mnb.score(fit_test2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6a3d748f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Texte'], dtype='object')\n",
      "382    RT @TierrasTaurinas: La loi europ√©enne donne l...\n",
      "303    C'est d'ailleurs un gros probl√®me. Le consomma...\n",
      "68     @MisterNounet @gouzou30 On a pas besoin de cen...\n",
      "432    Airbus livre 36 appareils en octobre @Airbus #...\n",
      "354    @HeleneThouy @PartiAnimaliste @TF1 Terrine et ...\n",
      "                             ...                        \n",
      "219    RT @greenpeacefr: Le nucl√©aire, garant de notr...\n",
      "213    @cdion @GoldbergNic @franceinter Vous avez rai...\n",
      "34     #nucleaire : pour pouvoir continuer √† rab√¢cher...\n",
      "229    RT @YvesPDB: Vaccin Covid-19 : \"L'AVIATION MON...\n",
      "437    Habitu√©s √† monopoliser la parole sur le #nucl√©...\n",
      "Name: Texte, Length: 145, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train.keys())\n",
    "print(X_train['Texte'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8dc6fbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 145)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "pipe_svd = make_pipeline(CountVectorizer(), TruncatedSVD(n_components=300))\n",
    "pipe_svd.fit(X_train['Texte'])\n",
    "fit_train_svd = pipe_svd.transform(X_train['Texte'])\n",
    "fit_train_svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b6af4da1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_62200\\646478927.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# Calculate the average Word2Vec vectors for each comment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m         \"\"\"\n\u001b[1;32m-> 1629\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   1953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1954\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1955\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1956\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m             \u001b[1;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mve\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m     \u001b[0mbinary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open_binary_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m     \u001b[0mdecompressed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mso_compression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_open_binary_stream\u001b[1;34m(uri, mode, transport_params)\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[0mscheme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sniff_scheme\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[0msubmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_transport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m     \u001b[0mfobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m         \u001b[0mfobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muri\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\smart_open\\local_file.py\u001b[0m in \u001b[0;36mopen_uri\u001b[1;34m(uri_as_string, mode, transport_params)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mopen_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri_as_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mparsed_uri\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri_as_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mfobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'uri_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin.gz'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "# Calculate the average Word2Vec vectors for each comment\n",
    "X = []\n",
    "for text in df['Texte']:\n",
    "    words = text.split()\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            vector = model[word]\n",
    "            vectors.append(vector)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if len(vectors) > 0:\n",
    "        X.append(np.mean(vectors, axis=0))\n",
    "    else:\n",
    "        X.append(np.zeros(300))\n",
    "\n",
    "# Transform the target variable 'Avis' into a numeric variable\n",
    "y = df['Avis'].apply(lambda x: 1 if x == 'positif' else 0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classifier using the Word2Vec vectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "da0cf486",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_62200\\1719039009.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# Fit the stacking classifier on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mstacking_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# Evaluate the performance of the stacking classifier on the testing data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"final_estimator_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;31m# base estimators will be used in transform, predict, and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;31m# predict_proba. They are exposed publicly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[0;32m    159\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_fit_single_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_estimators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\u001b[0m in \u001b[0;36m_fit_single_estimator\u001b[1;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1552\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1554\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   1555\u001b[0m                 \u001b[1;34m\"This solver needs samples of at least 2 classes\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1556\u001b[0m                 \u001b[1;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "stop_words = stopwords.words('english') + stopwords.words('french')\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(df_papers['Texte'])\n",
    "y = df_papers['Avis'].apply(lambda x: 1 if x == 'positif' else 0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the base models\n",
    "base_models = [('logreg', LogisticRegression(max_iter=1000)),\n",
    "               ('nb', MultinomialNB()),\n",
    "               ('svm', LinearSVC(max_iter=10000)),\n",
    "               ('rf', RandomForestClassifier())]\n",
    "\n",
    "# Define the meta-model\n",
    "meta_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Create the stacking classifier\n",
    "stacking_clf = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "# Fit the stacking classifier on the training data\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the stacking classifier on the testing data\n",
    "accuracy = stacking_clf.score(X_test, y_test)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff3a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

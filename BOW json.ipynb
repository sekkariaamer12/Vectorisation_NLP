{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5908971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_PATH = \"results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e33f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(CORPUS_PATH) as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "filtered_data = {key: value for key, value in data.items() if value.get('stance') != -1}\n",
    "\n",
    "# Save the filtered data back to a JSON file if needed\n",
    "filtered_data_path = 'filtered_corpus.json'\n",
    "with open(filtered_data_path, 'w') as file:\n",
    "    json.dump(filtered_data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d4ea1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(filtered_data_path) as i:\n",
    "    data = json.load(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6a35b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "import emoji\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Define custom stopwords for French and English\n",
    "custom_stopwords = set(stopwords.words('french') + stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "     #Convert emojis to text representation\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    # Remove non-alphanumeric characters except for certain characters that may form complete words\n",
    "    text = re.sub(r\"[^\\wÀ-ÿ\\s'-]\", '', text)\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Convert accented characters to their ASCII equivalent\n",
    "    text = unidecode.unidecode(text)\n",
    "\n",
    "\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c944553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amese\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text in the JSON data\n",
    "preprocessed_data = {}\n",
    "for key, value in data.items():\n",
    "    preprocessed_text = preprocess_text(value['text'])\n",
    "    preprocessed_data[key] = {'text': preprocessed_text, 'stance': value['stance']}  # Update the preprocessed text in the data\n",
    "\n",
    "# Save the preprocessed data back to a JSON file\n",
    "preprocessed_data_path = 'preprocessed_corpus.json'\n",
    "with open(preprocessed_data_path, 'w') as file:\n",
    "    json.dump(preprocessed_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e960ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yours', 'seront', 'by', 'c', 'mes', 'ain', 'aient', 'each', 'such', 'own', 'down', 'does', 'l', 'aurions', 'aurai', 't', 'out', 'his', 'my', 'off', 'any', 'serai', 'fussiez', 'too', 'can', 'eussiez', 'is', 'will', 'having', 'm', 'avais', 'same', 'avaient', 'ont', 'son', 'theirs', 'here', 'soyez', 'while', 'them', \"wouldn't\", 'him', 'an', 'other', 'un', 'fusse', 'et', 'ayons', 'tes', \"weren't\", 'eus', \"hasn't\", 'avions', 'weren', 'il', \"that'll\", 'eue', 'ours', 'aurait', 'over', 'what', 'was', 'fûtes', 'who', 'que', 'étions', 'auriez', 'under', 'étées', 'eu', 'o', 'do', 'not', 'eûtes', 'with', 'étantes', 'we', 'don', 'nor', 'been', 'ils', 'aurez', 'yourself', 'par', 'against', 'sois', 'of', 'nous', 'this', 'serais', 'hadn', 'serait', 'fut', 'most', 'yourselves', 'eussions', 'being', 'fusses', 'how', 'ses', 'avez', 'below', 'auraient', 'between', \"shouldn't\", 'la', 'isn', \"shan't\", 'at', 'aren', 'wasn', 'when', 'étiez', 'very', 'their', 'there', 'mon', 'some', 'j', 'for', 'ai', 'étés', 'couldn', 'serons', 'on', 'pas', 'a', \"hadn't\", \"isn't\", 'même', 'seras', 'auront', 'étée', 'mustn', 'avec', 'the', 'eût', 'herself', 'her', 'has', 'auras', 'but', 'ourselves', \"it's\", \"needn't\", 'ayant', 'sera', 'll', 'haven', 'through', 'they', 'votre', 'n', 'avait', 'themselves', 'soient', 'ayantes', 'sa', 'étais', 'me', 'about', 'ce', 'didn', \"you're\", 'as', 'te', 'you', 'fussions', 'au', 'est', \"wasn't\", 'ayante', 's', 'fûmes', 'doesn', 'sont', 've', 'dans', 'soyons', 'no', 'fût', 'étants', 'lui', 'ces', 'elle', \"should've\", 'mais', 'your', \"aren't\", 'above', 'he', \"couldn't\", 'because', 'if', 'une', 'it', 'be', \"won't\", 'once', 're', 'just', 'am', 'pour', \"she's\", 'fussent', 'only', 'soit', 'both', 'ne', 'after', 'en', 'fus', 'eux', \"haven't\", 'these', 'all', 'aux', 'aviez', 'where', 'mightn', 'or', 'should', 'seriez', 'wouldn', 'ou', 'était', \"doesn't\", 'eues', 'shan', 'to', 'notre', 'sommes', 'eûmes', 'shouldn', 'ait', 'serez', 'now', 'doing', 'eut', 'seraient', 'its', \"mightn't\", 'du', 'd', 'es', 'étante', 'étant', \"don't\", 'tu', 'se', 'ta', 'before', 'ma', 'qui', 'more', 'than', 'étaient', 'she', 'himself', 'further', 'ayez', 'that', 'did', 'serions', \"you'd\", 'êtes', 'itself', 'in', 'les', 'until', 'suis', 'why', 'je', 'aura', 'myself', 'again', 'des', 'needn', 'eussent', 'le', 'vos', 'vous', 'which', 'aurons', 'aie', \"didn't\", 'moi', 'nos', 'furent', 'ayants', 'were', 'de', 'those', 'during', 'hasn', 'aies', 'are', 'toi', 'y', 'sur', 'few', \"mustn't\", 'à', 'aurais', \"you've\", 'eusses', 'won', 'up', 'and', 'qu', 'eusse', 'i', 'hers', 'have', 'then', 'avons', 'leur', 'into', 'eurent', 'ton', \"you'll\", 'had', 'whom', 'from', 'our', 'so', 'été'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "# Une seule fois :\n",
    "if False : # Si déjà fit\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('words')\n",
    "    nltk.download('punkt')    \n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('brown')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('omw-1.4')   \n",
    "    \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "import string\n",
    "\n",
    "# Initialisation du \"Wordnet Lemmatizer\"\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#Pour tester :  print(lemmatizer.lemmatize(\"bats\"))\n",
    "\n",
    "\n",
    "from nltk.corpus import brown  # Il y a davantage de mots ici\n",
    "words = set(brown.words())\n",
    "\n",
    "#words = set(nltk.corpus.words.words()) : pas beaucoup de mots !\n",
    "stop_words=set(stopwords.words('english')+ stopwords.words('french')); # \";\" pour ne pas avoir les résultats !\n",
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26260751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amese\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amese\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from pattern.en import lemma\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english') + stopwords.words('french'))\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, text):\n",
    "    \n",
    "        lem_tokens = [lemmatizer.lemmatize(t.lower()) for t in word_tokenize(text) if\n",
    "                  t.lower() in words and\n",
    "                  t.lower() not in stop_words and\n",
    "                  # Remove punctuation and numbers\n",
    "                  re.match(r'[a-zA-Z]+', t)]\n",
    "        return (lem_tokens)\n",
    "\n",
    "#count_vec_lemmatize = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=stop_words, analyzer='word', \n",
    "                           # ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, token_pattern=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21e2f9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amese\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\amese\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['could', 'might', 'must', 'need', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a  315  termes dans le vocabulaire\n",
      "\n",
      "Les 50 premiers termes:\n",
      "['action' 'ad' 'aggressor' 'ah' 'aide' 'air' 'airline' 'alliance' 'alors'\n",
      " 'also' 'animal' 'annee' 'article' 'assurance' \"aujourd'hui\" 'avant'\n",
      " 'aviation' 'baron' 'base' 'bien' 'bomb' 'breaking' 'camera' 'candidate'\n",
      " 'capital' 'car' 'cargo' 'cause' 'certain' 'certification' 'chance'\n",
      " 'charge' 'chart' 'chef' 'chose' 'chute' 'citation' 'cite' 'civil' 'clean'\n",
      " 'cliche' 'code' 'coin' 'collection' 'combat' 'combustible' 'come'\n",
      " 'commence' 'comment' 'commercial']\n",
      "\n",
      "Tous les 50 termes:\n",
      "['action' 'commission' 'falcon' 'jeunes' 'pay' 'resilience' 'turkey']\n",
      "\n",
      "Le nbr d'occurrence des 50 premiers termes :\n",
      "[('airline', 6), ('camera', 22), ('hunt', 130), ('deux', 72), ('falcon', 100), ('royal', 258), ('air', 5), ('force', 109), ('provenance', 231), ('pay', 200), ('important', 138), ('car', 25), ('propose', 228), ('solution', 271), ('tout', 290), ('suite', 280), ('transport', 295), ('nest', 180), ('place', 205), ('decide', 65), ('jour', 151), ('plus', 208), ('aide', 4), ('production', 225), ('comment', 48), ('p', 192), ('presente', 222), ('impact', 135), ('cause', 27), ('via', 306), ('consequence', 56), ('preferable', 216), ('variable', 304), ('meme', 166), ('plan', 206), ('grandeur', 121), ('grand', 120), ('service', 266), ('avant', 15), ('bien', 19), ('point', 209), ('role', 256), ('suspend', 284), ('mobile', 175), ('candidate', 23), ('regulation', 244), ('capital', 24), ('implementation', 137), ('difficile', 73), ('manifestation', 163)]\n",
      "\n",
      "Le nbr d'occurrence des 50 derniers termes :\n",
      "[('humaine', 129), ('hors', 128), ('stable', 275), ('trace', 291), ('correspondent', 60), ('flexible', 105), ('chart', 32), ('pro', 224), ('punitive', 233), ('experience', 92), ('formation', 110), ('patron', 199), ('conversation', 58), ('presentation', 221), ('generation', 117), ('niger', 181), ('impose', 139), ('complete', 53), ('vrai', 311), ('improbable', 141), ('zero', 313), ('profit', 226), ('expert', 93), ('jeunes', 150), ('vis-a-vis', 309), ('russe', 260), ('debut', 64), ('evolution', 90), ('positive', 211), ('rate', 239), ('hurler', 131), ('declare', 66), ('chance', 30), ('clean', 39), ('sky', 269), ('cliche', 40), ('initial', 143), ('emission', 85), ('collection', 43), ('fake', 99), ('eh', 84), ('compatible', 52), ('coin', 42), ('ad', 1), ('pure', 234), ('usage', 302), ('hypocrite', 132), ('troll', 297), ('plaque', 207), ('bomb', 20)]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the preprocessed JSON file\n",
    "with open('preprocessed_corpus.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract the preprocessed text from the JSON\n",
    "texts = [value['text'] for value in data.values()]\n",
    "\n",
    "# Initialize and fit the CountVectorizer\n",
    "count_vec_lemmatise = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=stop_words, analyzer='word', \n",
    "                                      ngram_range=(1, 1), max_df=1.0, min_df=0, max_features=None)\n",
    "count_train = count_vec_lemmatise.fit(texts)\n",
    "\n",
    "bag_of_words = count_vec_lemmatise.transform(texts)\n",
    "\n",
    "# Quelques prints \n",
    "print(\"Il y a \", len(count_train.vocabulary_), \" termes dans le vocabulaire\\n\")\n",
    "\n",
    "# Print Les 50 premiers termes\n",
    "print(\"Les 50 premiers termes:\\n{}\".format(np.array(count_vec_lemmatise.get_feature_names_out()[:50])))\n",
    "print(\"\\nTous les 50 termes:\\n{}\".format(np.array(count_vec_lemmatise.get_feature_names_out()[::50]))) # Tous les 50 termes\n",
    "#print(type(count_train.vocabulary_))\n",
    "#print(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_))\n",
    "\n",
    "#les 50 premiers mots et leur nbr d'occurrence\n",
    "print(\"\\nLe nbr d'occurrence des 50 premiers termes :\")\n",
    "print([(k,v)  for k,v in count_train.vocabulary_.items()][:50])\n",
    "\n",
    "print(\"\\nLe nbr d'occurrence des 50 derniers termes :\")\n",
    "print([(k,v)  for k,v in count_train.vocabulary_.items()][-50:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7220d59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 174\n",
      "Test size: 20\n"
     ]
    }
   ],
   "source": [
    "with open('preprocessed_corpus.json') as i:\n",
    "    data = json.load(i)\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for key, value in data.items():\n",
    "    texts.append(value.get('text'))\n",
    "    labels.append(value.get('stance'))\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.1, random_state=1)\n",
    "\n",
    "print(\"Train size:\", len(train_texts))\n",
    "print(\"Test size:\", len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41e7ec00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count_vec_lemmatize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_94344\\4199396059.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcount_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vec_lemmatize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbag_of_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vec_lemmatize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'count_vec_lemmatize' is not defined"
     ]
    }
   ],
   "source": [
    "count_train = count_vec_lemmatize.fit(train_texts)\n",
    "bag_of_words = count_vec_lemmatize.transform(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_text = count_vec_lemmatize.fit(test_texts)\n",
    "bag_of_words_test = count_vec_lemmatize.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 features of the count_vec\n",
    "print(\"Les 50 premiers termes:\\n{}\".format(count_vec_lemmatize.get_feature_names()[:50]))\n",
    "print(\"\\nTous les 50 termes:\\n{}\".format(count_vec_lemmatize.get_feature_names()[::50])) # Tous les 50 termes\n",
    "\n",
    "# Print the number of features in the vocabulary\n",
    "print(\"\\nIl y a \", len(count_train.vocabulary_), \" termes dans le vocabulaire\\n\")\n",
    "\n",
    "# Print the count of the first 50 words\n",
    "print(\"Le nbr d'occurrence des 50 premiers termes :\")\n",
    "print([(k,v)  for k,v in count_train.vocabulary_.items()][:50]) \n",
    "\n",
    "# Print the count of the last 50 words\n",
    "print(\"Le nbr d'occurrence des 50 derniers termes :\")\n",
    "print([(k,v)  for k,v in count_train.vocabulary_.items()][-50:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "093a7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stop_words = set(stopwords.words('english') + stopwords.words('french'))\n",
    "\n",
    "TfIdf_lemmatise = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words=stop_words,\\\n",
    "                                  smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2af07abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_fitted = TfIdf_lemmatise.fit(train_texts,test_texts)\n",
    "train_lemmatise_transformed = corpus_fitted.transform(train_texts)\n",
    "test_lemmatise_transformed = corpus_fitted.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f8b8226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Les 10 dernières lignes de la matrice TfIdf (en ligne : les indices, en colonne les termes/mots) :\n",
      "     action   ad  aggressor   ah  aide  air  airline  alliance  alors  also  \\\n",
      "164     0.0  0.0        0.0  0.0   0.0  0.0      0.0       0.0    0.0   0.0   \n",
      "165     0.0  0.0        0.0  0.0   0.0  0.0      0.0       0.0    0.0   0.0   \n",
      "166     0.0  0.0        0.0  0.0   0.0  0.0      0.0       0.0    0.0   0.0   \n",
      "167     0.0  0.0        0.0  0.0   0.0  0.0      0.0       0.0    0.0   0.0   \n",
      "168     0.0  0.0        0.0  0.0   0.0  0.0      0.0       0.0    0.0   0.0   \n",
      "169     0.0  0.0        0.0  0.0   0.0  0.0      0.0       0.0    0.0   0.0   \n",
      "170     0.0  0.0        0.0  0.0   0.0  0.0      0.0       0.0    0.0   0.0   \n",
      "171     0.0  0.0        0.0  0.0   0.0  0.0      0.0       0.0    0.0   0.0   \n",
      "172     0.0  0.0        0.0  0.0   0.0  0.0      0.0       0.0    0.0   0.0   \n",
      "173     0.0  0.0        0.0  0.0   0.0  0.0      0.0       0.0    0.0   0.0   \n",
      "\n",
      "     ...  variable  vent       via  video  vies  vis-a-vis  vise  vrai  world  \\\n",
      "164  ...       0.0   0.0  5.465908    0.0   0.0        0.0   0.0   0.0    0.0   \n",
      "165  ...       0.0   0.0  0.000000    0.0   0.0        0.0   0.0   0.0    0.0   \n",
      "166  ...       0.0   0.0  0.000000    0.0   0.0        0.0   0.0   0.0    0.0   \n",
      "167  ...       0.0   0.0  0.000000    0.0   0.0        0.0   0.0   0.0    0.0   \n",
      "168  ...       0.0   0.0  0.000000    0.0   0.0        0.0   0.0   0.0    0.0   \n",
      "169  ...       0.0   0.0  0.000000    0.0   0.0        0.0   0.0   0.0    0.0   \n",
      "170  ...       0.0   0.0  0.000000    0.0   0.0        0.0   0.0   0.0    0.0   \n",
      "171  ...       0.0   0.0  0.000000    0.0   0.0        0.0   0.0   0.0    0.0   \n",
      "172  ...       0.0   0.0  0.000000    0.0   0.0        0.0   0.0   0.0    0.0   \n",
      "173  ...       0.0   0.0  0.000000    0.0   0.0        0.0   0.0   0.0    0.0   \n",
      "\n",
      "     zone  \n",
      "164   0.0  \n",
      "165   0.0  \n",
      "166   0.0  \n",
      "167   0.0  \n",
      "168   0.0  \n",
      "169   0.0  \n",
      "170   0.0  \n",
      "171   0.0  \n",
      "172   0.0  \n",
      "173   0.0  \n",
      "\n",
      "[10 rows x 288 columns]\n",
      "--------------------------------------------------\n",
      "2) Quelques indice Idf pour certains mots :\n",
      "           idf_weights\n",
      "action        6.159055\n",
      "ad            6.159055\n",
      "aggressor     6.159055\n",
      "ah            5.465908\n",
      "aide          6.159055\n",
      "...                ...\n",
      "vis-a-vis     6.159055\n",
      "vise          5.060443\n",
      "vrai          6.159055\n",
      "world         6.159055\n",
      "zone          6.159055\n",
      "\n",
      "[288 rows x 1 columns]\n",
      "--------------------------------------------------\n",
      "2bis) Le 100 premiers termes : colonnes de la matrice TfIdf\n",
      "['action' 'ad' 'aggressor' 'ah' 'aide' 'air' 'airline' 'alliance' 'alors'\n",
      " 'also' 'animal' 'annee' 'article' 'assurance' \"aujourd'hui\" 'avant'\n",
      " 'aviation' 'baron' 'base' 'bien' 'bomb' 'breaking' 'camera' 'capital'\n",
      " 'car' 'cargo' 'cause' 'certain' 'certification' 'chance' 'charge' 'chose'\n",
      " 'chute' 'citation' 'cite' 'civil' 'clean' 'cliche' 'code' 'coin'\n",
      " 'collection' 'combat' 'combustible' 'come' 'commence' 'comment'\n",
      " 'commercial' 'commission' 'compare' 'compatible' 'complete' 'concept'\n",
      " 'condition' 'consequence' 'conversation' 'conviction' 'coup' \"d'un\"\n",
      " 'danger' 'debut' 'decide' 'declare' 'defend' 'defense' 'deja' 'depart'\n",
      " 'derriere' 'deux' 'difficile' 'dire' 'disposition' 'documentary' 'dollar'\n",
      " 'drone' 'dun' 'dune' 'durable' 'e' 'eating' 'eh' 'emission' 'energy'\n",
      " 'engagement' 'eternity' 'etes' 'evolution' 'exhibition' 'experience'\n",
      " 'expert' 'exploitation' 'explosion' 'expose' 'fabrication' 'face' 'fake'\n",
      " 'falcon' 'famille' 'film' 'fin' 'floc']\n",
      "--------------------------------------------------\n",
      "3) Le 100 derniers termes de la matrice = colonnes de la matrice\n",
      "['zone' 'world' 'vrai' 'vise' 'vis-a-vis' 'vies' 'video' 'via' 'vent'\n",
      " 'variable' 'v' 'usage' 'type' 'turkey' 'tt' 'tropical' 'troll' 'tribunal'\n",
      " 'transport' 'transition' 'train' 'tradition' 'tout' 'tour' 'titre'\n",
      " 'technique' 'suspend' 'surtout' 'surface' 'super' 'suite' 'sub'\n",
      " 'structure' 'steak' 'star' 'stable' 'spring' 'source' 'sortie' 'solution'\n",
      " 'social' 'sky' 'site' 'simple' 'service' 'segment' 'scenario' 'saint'\n",
      " 'russe' 'rupture' 'royal' 'route' 'role' 'rocket' 'rich' 'revolution'\n",
      " 'reunion' 'retard' 'resilience' 'repose' 'reportage' 'rend' 'religion'\n",
      " 'relation' 'regulation' 'regional' 'refuse' 'reduction' 'ready' 'rare'\n",
      " 'rapport' 'quota' 'question' 'pure' 'punitive' 'provenance' 'protege'\n",
      " 'propose' 'propagation' 'profit' 'production' 'pro' 'privilege'\n",
      " 'presente' 'presentation' 'presence' 'prepare' 'premier' 'preference'\n",
      " 'preferable' 'precedent' 'possible' 'positive' 'porter' 'point' 'plus'\n",
      " 'plaque' 'plan' 'place' 'pioneer']\n",
      "--------------------------------------------------\n",
      "Il y a 288 termes.\n",
      "4) La matrice sur X_train :\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Une partie de la matrice TDIDF\n",
    "temp_df = pd.DataFrame(train_lemmatise_transformed.toarray(), columns=TfIdf_lemmatise.get_feature_names_out())\n",
    "\n",
    "print(\"1) Les 10 dernières lignes de la matrice TfIdf (en ligne : les indices, en colonne les termes/mots) :\")\n",
    "print(temp_df.tail(10))\n",
    "print(\"-\"*50)\n",
    "\n",
    "df_idf = pd.DataFrame(TfIdf_lemmatise.idf_, index=TfIdf_lemmatise.get_feature_names_out(),columns=[\"idf_weights\"])\n",
    "print(\"2) Quelques indice Idf pour certains mots :\")\n",
    "print(df_idf)\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"2bis) Le 100 premiers termes : colonnes de la matrice TfIdf\")\n",
    "liste_termes = TfIdf_lemmatise.get_feature_names_out()\n",
    "print(liste_termes[:100])\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"3) Le 100 derniers termes de la matrice = colonnes de la matrice\")\n",
    "liste_termes_reversed = liste_termes[::-1]\n",
    "print(liste_termes_reversed[:100])\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"Il y a\", len(liste_termes), \"termes.\")\n",
    "print(\"4) La matrice sur X_train :\\n\", train_lemmatise_transformed.toarray())\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "934a79f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((174, 288), (20, 288))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_train=train_lemmatise_transformed\n",
    "fit_test = test_lemmatise_transformed\n",
    "fit_train.shape, fit_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95ed36e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174, 1982)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "    # On a fait le travaille plus haut et déjà créer fit_train et fit_test\n",
    "pipe = make_pipeline(CountVectorizer(), TfidfTransformer()) # Construire un Construct a Pipeline from the given estimators.\n",
    "pipe.fit(train_texts)\n",
    "\n",
    "fit_train = pipe.transform(train_texts)\n",
    "fit_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01089082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1982)\n"
     ]
    }
   ],
   "source": [
    "fit_test = pipe.transform(test_texts)\n",
    "print(fit_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59484101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=train_labels\n",
    "y_test=test_labels\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(fit_train, y_train)\n",
    "dt.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55b74a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "clf.fit(fit_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dbd6ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbd96e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(fit_train, y_train)\n",
    "lr.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a58f75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(fit_train, y_train)\n",
    "mnb.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38f48061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "modele_one_vs_linear_SVC= OneVsRestClassifier(LinearSVC())\n",
    "modele_one_vs_linear_SVC.fit(fit_train, y_train)\n",
    "modele_one_vs_linear_SVC.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "129fd544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "modele_one_vs_SVC = OneVsRestClassifier(SVC())\n",
    "modele_one_vs_SVC.fit(fit_train, y_train)\n",
    "modele_one_vs_SVC.score(fit_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "963ced20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174, 288)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amese\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['could', 'might', 'must', 'need', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe_bis = make_pipeline(CountVectorizer(tokenizer=LemmaTokenizer(), stop_words=stop_words, analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None), TfidfTransformer())\n",
    "pipe_bis.fit(train_texts)\n",
    "fit_train_bis = pipe_bis.transform(train_texts)\n",
    "print(fit_train_bis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa609d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 288)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_test_bis = pipe_bis.transform(test_texts)\n",
    "fit_test_bis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a78d2caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(fit_train_bis, y_train)\n",
    "dt.score(fit_test_bis, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27e51299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=70)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=70)\n",
    "clf.fit(fit_train_bis, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a30fb90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(fit_test_bis, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e4dfadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(fit_train_bis, y_train)\n",
    "lr.score(fit_test_bis, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2afd1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(fit_train_bis, y_train)\n",
    "mnb.score(fit_test_bis, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7754061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "modele_one_vs_linear_SVC= OneVsRestClassifier(LinearSVC())\n",
    "modele_one_vs_linear_SVC.fit(fit_train_bis, y_train)\n",
    "modele_one_vs_linear_SVC.score(fit_test_bis, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f26e26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, None, 'RT'), (None, 'RT', 'SismiquePodcast'), ('RT', 'SismiquePodcast', 'Y-a-t-il'), ('SismiquePodcast', 'Y-a-t-il', 'des'), ('Y-a-t-il', 'des', 'limites')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "generated_ngrams = list(ngrams(word_tokenize(train_texts[0]), 3, pad_left=True, pad_right=True))\n",
    "print(generated_ngrams[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "301bd590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174, 6034)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "pipe2 = make_pipeline(CountVectorizer(ngram_range=(1, 2)), TfidfTransformer())\n",
    "pipe2.fit(train_texts)\n",
    "fit_train2 = pipe2.transform(train_texts)\n",
    "fit_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6dd8baa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 6034)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_test2 = pipe2.transform(test_texts)\n",
    "fit_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdf2e087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf2 = LogisticRegression(solver='lbfgs')\n",
    "clf2.fit(fit_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a897ba76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.score(fit_test2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "abec3760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(cv=5)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf2_ = LogisticRegressionCV(cv=5)\n",
    "clf2_.fit(fit_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00fb8772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2_.score(fit_test2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "272fd3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(fit_train2, y_train)\n",
    "mnb.score(fit_test2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f70982d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "pipe_svd_tfidf = make_pipeline(CountVectorizer(), \n",
    "                     TfidfTransformer(),\n",
    "                     TruncatedSVD(n_components=500))\n",
    "pipe_svd_tfidf.fit(train_texts)\n",
    "fit_train_svd_tfidf = pipe_svd_tfidf.transform(train_texts)\n",
    "\n",
    "clf_svd_tfidf = LogisticRegression(solver='lbfgs',max_iter=200)\n",
    "clf_svd_tfidf.fit(fit_train_svd_tfidf, y_train)\n",
    "\n",
    "fit_test_svd_tfidf = pipe_svd_tfidf.transform(test_texts)\n",
    "clf_svd_tfidf.score(fit_test_svd_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae973fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
